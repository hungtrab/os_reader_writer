\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{enumitem}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}

% Code listing settings
\lstset{
    language=C,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray}
}

\lstdefinestyle{bash}{
    language=bash,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black}
}

\title{\textbf{The Reader-Writer Problem:} \\
\large Implementation and Analysis of Synchronization Strategies}

\author{
Tran Quang Hung - 20235502\\
Nguyen Xuan Khai - 20235508\\
\\
\textit{Hanoi University of Science and Technology}\\
\textit{Supervisor: PhD. Do Quoc Huy}
}

\date{\today}

\begin{document}

% Custom Title Page with Logo
\begin{titlepage}
    \begin{center}
        % Logo SOICT HUST
        \includegraphics[width=0.8\textwidth]{media/soict_hust_logo.png}
        
        \vspace{0.5cm}
        {\Large \textbf{HANOI UNIVERSITY OF SCIENCE AND TECHNOLOGY}}\\[0.5cm]
        {\large School of Information and Communication Technology}\\[0.3cm]
        \rule{\textwidth}{0.5pt}
        
        \vspace{2cm}
        
        % Title
        {\huge \textbf{The Reader-Writer Problem:}}\\[0.5cm]
        {\LARGE Implementation and Analysis of Synchronization Strategies}
        
        \vspace{1.5cm}
        
        % Course/Subject info
        {\large \textbf{Operating Systems - IT3070E}}\\[0.3cm]
        {\large Topic 6: Readers-Writers Problem}
        
        \vspace{1.5cm}
        
        % Authors
        \begin{tabular}{rl}
            \textbf{Students:} & Tran Quang Hung - 20235502 \\
                              & Nguyen Xuan Khai - 20235508 \\[0.5cm]
            \textbf{Supervisor:} & PhD. Do Quoc Huy \\[0.5cm]
            \textbf{Class:} & 161859
        \end{tabular}
        
        \vfill
        
        % Bottom - Date
        {\large Hanoi, \today}
    \end{center}
\end{titlepage}

% Abstract on new page
\newpage

\begin{abstract}
The Reader-Writer problem represents one of the fundamental challenges in concurrent programming and operating systems design. This paper presents a comprehensive implementation and analysis of four distinct synchronization strategies: vanilla (no synchronization), reader preference, writer preference, and fair scheduling using the turnstile pattern. We implement a shared string manipulation application to demonstrate the behavior and trade-offs of each approach. Through extensive automated testing (16 runs across 4 modes), we demonstrate race conditions in unsynchronized access (average 367 torn reads in vanilla mode) and perfect correctness in synchronized modes (0 errors in 12 runs). This work provides both theoretical analysis and practical implementation insights valuable for concurrent systems design.
\end{abstract}

\section{Introduction}

\subsection{Motivation}

In modern computing systems, concurrent access to shared resources is ubiquitous. Database systems must handle simultaneous read and write transactions, file systems must coordinate multiple processes accessing the same files, and cache systems must maintain consistency while allowing parallel reads for performance. The Reader-Writer problem encapsulates this fundamental challenge: how to allow multiple concurrent readers while ensuring exclusive access for writers, all while maintaining correctness, performance, and fairness.

The importance of this problem extends beyond academic interest. Real-world systems such as PostgreSQL, Linux kernel read-write semaphores, and Java's \texttt{ReadWriteLock} all implement solutions to this problem. Understanding the trade-offs between different synchronization strategies is crucial for systems engineers and application developers.

\subsection{Problem Statement}

The classical Reader-Writer problem involves multiple concurrent threads accessing a shared resource, where:

\begin{itemize}
    \item \textbf{Readers} only read data and can operate concurrently without interfering with each other
    \item \textbf{Writers} modify data and require exclusive access (no other readers or writers)
\end{itemize}

The challenge is to design a synchronization mechanism that:
\begin{enumerate}
    \item Ensures \textbf{correctness}: No data corruption from concurrent access
    \item Maximizes \textbf{performance}: Allow as much concurrency as safely possible
    \item Prevents \textbf{starvation}: No thread waits indefinitely
    \item Provides \textbf{fairness}: Balanced access for all threads
\end{enumerate}

These requirements often conflict, creating fundamental trade-offs that must be carefully balanced based on workload characteristics.

\subsection{Contributions}

This paper makes the following contributions:

\begin{itemize}
    \item A unified implementation framework supporting four synchronization modes with a single API
    \item Comprehensive automated testing demonstrating race conditions and validating correctness
    \item Quantitative analysis of performance trade-offs between different synchronization strategies
    \item Practical insights for implementing Reader-Writer locks in real systems
\end{itemize}

\section{Background}

\subsection{The Reader-Writer Problem in Theory}

The Reader-Writer problem was first formally described by Courtois, Heymans, and Parnas in 1971. It represents a fundamental synchronization challenge where:

\begin{itemize}
    \item Multiple readers can access shared data concurrently (read operations are non-interfering)
    \item Writers require exclusive access (no other readers or writers during write)
    \item Readers and writers must coordinate to prevent data corruption
\end{itemize}

\textbf{Formal Requirements:}

\begin{enumerate}
    \item \textbf{Mutual Exclusion for Writers}: At most one writer can access the resource at any time
    \item \textbf{Concurrent Readers}: Multiple readers can access simultaneously
    \item \textbf{No Reader-Writer Concurrency}: Writers must have exclusive access
    \item \textbf{Progress}: Both readers and writers should eventually gain access
    \item \textbf{Bounded Waiting}: No indefinite postponement (depends on policy)
\end{enumerate}

\subsection{Real-World Applications}

\textbf{Database Management Systems:}
\begin{itemize}
    \item PostgreSQL uses Multi-Version Concurrency Control (MVCC) with reader-writer semantics
    \item MySQL InnoDB implements row-level read-write locks
    \item Read transactions (SELECT) can run concurrently
    \item Write transactions (INSERT/UPDATE/DELETE) require exclusive access
\end{itemize}

\textbf{Operating Systems:}
\begin{itemize}
    \item Linux kernel \texttt{rwlock\_t} for shared data structures
    \item File system metadata access (directory listings vs. file modifications)
    \item Process synchronization in shared memory segments
    \item Cache coherence protocols in multiprocessor systems
\end{itemize}

\textbf{Programming Languages:}
\begin{itemize}
    \item Java: \texttt{ReentrantReadWriteLock} in \texttt{java.util.concurrent}
    \item C++17: \texttt{std::shared\_mutex} for reader-writer scenarios
    \item Python: \texttt{RWLock} implementations in threading libraries
    \item Rust: \texttt{RwLock<T>} providing safe concurrent access
\end{itemize}

\textbf{Web Services:}
\begin{itemize}
    \item Configuration caches (frequent reads, rare updates)
    \item Session storage (many reads per user, occasional writes)
    \item Content delivery networks (cache reads vs. purge operations)
\end{itemize}

\subsection{Overall Architecture}

Our system consists of three layers:

\begin{enumerate}
    \item \textbf{Common Infrastructure Layer}: Provides unified synchronization API and logging utilities
    \item \textbf{Application Layer}: Shared string manipulation demonstrating concurrent access
    \item \textbf{Configuration Layer}: CLI argument parsing and runtime parameter management
\end{enumerate}

This separation ensures modularity and makes it easy to extend the system with additional test scenarios.

\subsection{Synchronization Primitives: Mutex vs. Semaphore}

The Reader-Writer problem can be implemented using different synchronization primitives. Understanding the differences is crucial for correct implementation.

\textbf{Semaphore-based Solutions:}
\begin{itemize}
    \item Use counting semaphores to control access
    \item Binary semaphore (value 0 or 1) acts like a mutex
    \item Example: \texttt{sem\_wait()} and \texttt{sem\_post()} operations
    \item Advantage: Simpler conceptual model for some algorithms
    \item Common in classic textbook implementations
\end{itemize}

\textbf{Mutex-based Solutions (Our Implementation):}
\begin{itemize}
    \item Use \texttt{pthread\_mutex\_t} for mutual exclusion
    \item Better integration with POSIX condition variables
    \item Clearer ownership semantics (thread that locks must unlock)
    \item Widely supported across platforms
    \item Easier debugging (can track which thread owns lock)
\end{itemize}

\textbf{Critical Clarification:}

It's essential to understand that:
\begin{itemize}
    \item \textbf{Mutex and Semaphore} are \textit{synchronization primitives} (building blocks)
    \item \textbf{Reader Preference, Writer Preference, Fair Scheduling} are \textit{algorithms} (strategies)
    \item The same algorithm can be implemented using different primitives
    \item Our choice of mutex vs. semaphore doesn't change the fundamental algorithm
\end{itemize}

\subsection{Why POSIX Threads?}

Our implementation uses POSIX threads (pthreads), which provides:
\begin{itemize}
    \item \textbf{Mutexes}: For mutual exclusion (\texttt{pthread\_mutex\_t})
    \item \textbf{Condition Variables}: For thread coordination (\texttt{pthread\_cond\_t})
    \item \textbf{Thread Management}: Creation, joining, cancellation
    \item \textbf{Platform Independence}: Works on Linux, Unix, macOS
    \item \textbf{Industry Standard}: Widely used in production systems
\end{itemize}

\subsection{Unified Lock API}

We designed a unified API that supports all four synchronization modes with a single interface:

\begin{lstlisting}
typedef enum {
    VANILLA,      // No synchronization
    READER_PREF,  // Reader preference
    WRITER_PREF,  // Writer preference
    FAIR          // Fair scheduling
} rw_mode_t;

typedef struct {
    rw_mode_t mode;
    int active_readers;    // Currently reading
    int active_writers;    // Currently writing (0 or 1)
    int waiting_writers;   // Waiting to write
    pthread_mutex_t mutex;         // Protects counters
    pthread_mutex_t resource_lock; // Protects shared resource
    pthread_mutex_t read_try;      // For writer preference
    pthread_mutex_t queue_lock;    // For fair mode (turnstile)
} rw_lock_t;

// API Functions
void rw_init(rw_lock_t *lock, rw_mode_t mode);
void reader_enter(rw_lock_t *lock);
void reader_exit(rw_lock_t *lock);
void writer_enter(rw_lock_t *lock);
void writer_exit(rw_lock_t *lock);
void rw_destroy(rw_lock_t *lock);
\end{lstlisting}

\textbf{Design Benefits:}

\begin{enumerate}
    \item \textbf{Mode Isolation}: Each mode's logic is contained in switch-case statements
    \item \textbf{Easy Comparison}: Switch modes with a single parameter change
    \item \textbf{Consistent Interface}: Same API regardless of synchronization strategy
    \item \textbf{Performance Fairness}: All modes use same infrastructure, differences are purely algorithmic
\end{enumerate}

This design allows easy comparison between modes by simply changing the mode parameter, ensuring that performance differences are due to the synchronization strategy rather than implementation variations.

\section{Implementation Details}

\subsection{Shared String Manipulation}

\subsubsection{Overview}

This implementation demonstrates \textit{torn reads}--partial visibility of updates.

\textbf{Shared Resource:} A character array \texttt{char shared\_string[256]}

\textbf{Writer Behavior:} Writers cycle through predefined sentences:

\begin{lstlisting}
writer_enter(&lock);
if (mode == VANILLA) {
    // Copy char-by-char with delay (to increase race probability)
    for (int i = 0; sentence[i] != '\0'; i++) {
        shared_string[i] = sentence[i];
        usleep(100);  // Intentional delay
    }
} else {
    strncpy(shared_string, sentence, STRING_SIZE - 1);
}
writer_exit(&lock);
\end{lstlisting}

\textbf{Reader Behavior:} Readers read and validate strings:

\begin{lstlisting}
reader_enter(&lock);
strncpy(buffer, shared_string, STRING_SIZE - 1);
reader_exit(&lock);

// Validate against known valid sentences
if (string_not_in_valid_set(buffer)) {
    // TORN READ DETECTED!
}
\end{lstlisting}

\subsubsection{Race Condition Demonstration}

Without synchronization, readers can observe \textit{torn reads}--strings that are partially updated:

\textbf{Example Timeline:}
\begin{verbatim}
t=0:  shared_string = "Hello World!"
t=1:  W1 starts writing "Operating systems..."
      shared_string = "Oerating World!"  (partial)
t=2:  R1 reads: "Oerating World!"  <- TORN READ!
t=3:  W1 continues...
\end{verbatim}

These torn reads represent real data corruption that would occur in production systems without proper synchronization.

\section{Synchronization Algorithms}

\subsection{Mode 1: Vanilla (No Synchronization)}

\textbf{Purpose:} Baseline to demonstrate race conditions.

\begin{lstlisting}
void reader_enter(rw_lock_t *lock) {
    if (lock->mode == VANILLA) {
        lock->active_readers++;  // NO LOCK!
        return;
    }
}
\end{lstlisting}

This mode deliberately has no synchronization to demonstrate the problem. \textbf{Never use in production.}

\subsection{Mode 2: Reader Preference}

\textbf{Algorithm:} First reader locks resource, subsequent readers only increment counter.

\begin{lstlisting}
void reader_enter(rw_lock_t *lock) {
    pthread_mutex_lock(&lock->mutex);
    lock->active_readers++;
    if (lock->active_readers == 1) {
        pthread_mutex_lock(&lock->resource_lock);
    }
    pthread_mutex_unlock(&lock->mutex);
}

void reader_exit(rw_lock_t *lock) {
    pthread_mutex_lock(&lock->mutex);
    lock->active_readers--;
    if (lock->active_readers == 0) {
        pthread_mutex_unlock(&lock->resource_lock);
    }
    pthread_mutex_unlock(&lock->mutex);
}
\end{lstlisting}

\textbf{Advantages:}
\begin{itemize}
    \item Maximizes read throughput
    \item Multiple readers operate concurrently
    \item Low reader latency
\end{itemize}

\textbf{Disadvantage:} Writer starvation--if readers continuously arrive, writers may wait indefinitely.

\subsection{Mode 3: Writer Preference}

\textbf{Algorithm:} Writers block new readers using \texttt{read\_try} lock.

\begin{lstlisting}
void writer_enter(rw_lock_t *lock) {
    pthread_mutex_lock(&lock->mutex);
    lock->waiting_writers++;
    pthread_mutex_unlock(&lock->mutex);
    
    pthread_mutex_lock(&lock->read_try);  // Block new readers
    pthread_mutex_lock(&lock->resource_lock);
    
    pthread_mutex_lock(&lock->mutex);
    lock->waiting_writers--;
    pthread_mutex_unlock(&lock->mutex);
}

void reader_enter(rw_lock_t *lock) {
    pthread_mutex_lock(&lock->read_try);  // Must acquire first
    pthread_mutex_lock(&lock->mutex);
    lock->active_readers++;
    if (lock->active_readers == 1) {
        pthread_mutex_lock(&lock->resource_lock);
    }
    pthread_mutex_unlock(&lock->mutex);
    pthread_mutex_unlock(&lock->read_try);
}
\end{lstlisting}

\textbf{Advantages:}
\begin{itemize}
    \item Prevents writer starvation
    \item Ensures timely updates
\end{itemize}

\textbf{Disadvantage:} Reader starvation--continuous writers can delay readers significantly.

\subsection{Mode 4: Fair Scheduling (Turnstile Pattern)}

\textbf{Algorithm:} All threads pass through a \texttt{queue\_lock} "turnstile" ensuring FIFO ordering.

\begin{lstlisting}
void reader_enter(rw_lock_t *lock) {
    pthread_mutex_lock(&lock->queue_lock);  // Turnstile
    pthread_mutex_lock(&lock->mutex);
    lock->active_readers++;
    if (lock->active_readers == 1) {
        pthread_mutex_lock(&lock->resource_lock);
    }
    pthread_mutex_unlock(&lock->mutex);
    pthread_mutex_unlock(&lock->queue_lock);
}

void writer_enter(rw_lock_t *lock) {
    pthread_mutex_lock(&lock->queue_lock);  // Turnstile
    pthread_mutex_lock(&lock->resource_lock);
    pthread_mutex_unlock(&lock->queue_lock);
}
\end{lstlisting}

\textbf{Advantages:}
\begin{itemize}
    \item No starvation for either readers or writers
    \item FIFO ordering ensures fairness
    \item Predictable latency
\end{itemize}

\textbf{Disadvantage:} Slight throughput reduction due to extra lock overhead.

\section{Detailed Algorithm Analysis and Implementation Insights}

\subsection{Comprehensive Strategy Comparison}

\begin{table}[H]
\centering
\caption{Detailed Mode Comparison Matrix}
\small
\begin{tabular}{|l|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
\hline
\textbf{Property} & \textbf{Vanilla} & \textbf{Read Pref} & \textbf{Write Pref} & \textbf{Fair} \\ \hline
Data Integrity & X None & OK Full & OK Full & OK Full \\ \hline
Read Concurrency & Yes (unsafe) & Yes (safe) & Yes (safe) & Yes (safe) \\ \hline
Write Exclusivity & X No & OK Yes & OK Yes & OK Yes \\ \hline
Reader Starvation Risk & None & None & High & None \\ \hline
Writer Starvation Risk & None & High & None & None \\ \hline
Lock Overhead & 0 mutexes & 2 mutexes & 3 mutexes & 3 mutexes \\ \hline
Code Complexity & Trivial & Low & Medium & Medium \\ \hline
Best Use Case & None (demo) & 90\%+ reads & Write-heavy & Mixed \\ \hline
\end{tabular}
\end{table}

\subsection{Lock Acquisition Ordering}

One critical aspect of correct implementation is consistent lock ordering to prevent deadlocks.

\textbf{Lock Hierarchy (must be respected):}
\begin{enumerate}
    \item \texttt{queue\_lock} (Fair mode turnstile) - highest priority
    \item \texttt{read\_try} (Writer Preference reader blocker)
    \item \texttt{mutex} (Counter protection)
    \item \texttt{resource\_lock} (Shared resource protection) - lowest priority
\end{enumerate}

Violating this order can cause deadlock between threads.

\subsection{Why the Counter Protection Mutex is Essential}

Students often ask: "Why do we need \texttt{mutex} if we already have \texttt{resource\_lock}?"

\textbf{Answer:} They serve different purposes!

\begin{itemize}
    \item \texttt{resource\_lock}: Protects the shared data (\texttt{shared\_string})
    \item \texttt{mutex}: Protects the synchronization logic (counters and conditions)
\end{itemize}

\textbf{Broken Implementation Without \texttt{mutex}:}

\begin{lstlisting}
void reader_enter(rw_lock_t *lock) {
    // WRONG - no mutex protection!
    lock->active_readers++;  // Race condition here!
    if (lock->active_readers == 1) {
        pthread_mutex_lock(&lock->resource_lock);
    }
}
\end{lstlisting}

\textbf{Failure Scenario Timeline:}

\begin{table}[H]
\centering
\caption{Race Condition on Counter Without Mutex}
\begin{tabular}{|c|l|l|c|}
\hline
\textbf{Time} & \textbf{Reader 1} & \textbf{Reader 2} & \textbf{active\_readers} \\ \hline
$t_0$ & & & 0 \\ \hline
$t_1$ & LOAD active\_readers (0) & & 0 \\ \hline
$t_2$ & & LOAD active\_readers (0) & 0 \\ \hline
$t_3$ & ADD 1 $\rightarrow$ 1 & & 0 \\ \hline
$t_4$ & & ADD 1 $\rightarrow$ 1 & 0 \\ \hline
$t_5$ & STORE 1 & & 1 \\ \hline
$t_6$ & & STORE 1 & 1 (WRONG!) \\ \hline
$t_7$ & if (1 == 1) TRUE & & 1 \\ \hline
$t_8$ & & if (1 == 1) TRUE & 1 \\ \hline
$t_9$ & lock(resource\_lock) OK & & 1 \\ \hline
$t_{10}$ & & lock(resource\_lock) DEADLOCK! & 1 \\ \hline
\end{tabular}
\end{table}

\textbf{The Problem:} Both readers think they are the "first" reader, both try to lock \texttt{resource\_lock}.

\textbf{The Solution:} Use \texttt{mutex} to make the entire check-and-lock sequence atomic:

\begin{lstlisting}
void reader_enter(rw_lock_t *lock) {
    pthread_mutex_lock(&lock->mutex);  // Atomicity start
    lock->active_readers++;
    if (lock->active_readers == 1) {
        pthread_mutex_lock(&lock->resource_lock);
    }
    pthread_mutex_unlock(&lock->mutex);  // Atomicity end
}
\end{lstlisting}

Now only ONE thread sees \texttt{active\_readers == 1}, preventing the double-lock scenario.

\subsection{Turnstile Pattern Deep Dive}

The turnstile pattern is elegant but often confusing. Let's analyze it thoroughly.

\textbf{Physical Analogy:} Imagine a subway turnstile:
\begin{itemize}
    \item People queue in arrival order
    \item Each person must pass through the turnstile one at a time
    \item The turnstile itself doesn't grant access to the train (that's the \texttt{resource\_lock})
    \item It just ensures FIFO ordering
\end{itemize}

\textbf{Code Implementation:}

\begin{lstlisting}
void reader_enter(rw_lock_t *lock) {
    pthread_mutex_lock(&lock->queue_lock);    // [A] Enter turnstile
    pthread_mutex_lock(&lock->mutex);         // [B] Access counters
    lock->active_readers++;
    if (lock->active_readers == 1) {
        pthread_mutex_lock(&lock->resource_lock); // [C] First reader locks resource
    }
    pthread_mutex_unlock(&lock->mutex);       // [D] Release counters
    pthread_mutex_unlock(&lock->queue_lock);  // [E] Exit turnstile
    // Now reading...
}
\end{lstlisting}

\textbf{Key Insight:} Steps [B], [C], [D] happen while holding \texttt{queue\_lock}. This means no other thread (reader OR writer) can start these steps until current thread completes them.

\textbf{Fairness Demonstration:}

Arrival sequence: R1 → W1 → R2

\textit{Without Turnstile (Reader Preference):}
\begin{enumerate}
    \item R1 enters, \texttt{active\_readers = 1}, locks \texttt{resource\_lock}
    \item W1 tries to lock \texttt{resource\_lock}, BLOCKS
    \item R2 enters, \texttt{active\_readers = 2} (doesn't need \texttt{resource\_lock})
    \item R2 starts reading even though W1 was waiting! UNFAIR!
\end{enumerate}

\textit{With Turnstile (Fair):}
\begin{enumerate}
    \item R1 locks \texttt{queue\_lock}, enters, unlocks \texttt{queue\_lock}
    \item W1 tries \texttt{queue\_lock}, must wait for R1
    \item R1 finishes setup, releases \texttt{queue\_lock}
    \item W1 gets \texttt{queue\_lock}, but R1 still reading
    \item R2 tries \texttt{queue\_lock}, BLOCKS (W1 owns it)
    \item R1 finishes, W1 can now lock \texttt{resource\_lock}
    \item W1 finishes, releases \texttt{queue\_lock}
    \item R2 gets \texttt{queue\_lock} → FAIR!
\end{enumerate}

\subsection{Implementation Challenges}\subsubsection{Challenge 1: Avoiding Deadlock}

With multiple locks, deadlock is a real risk. Our solution: consistent lock ordering.

\textbf{Deadlock Scenario:}
\begin{verbatim}
Thread A: lock(X) -> lock(Y)
Thread B: lock(Y) -> lock(X)  // DEADLOCK!
\end{verbatim}

\textbf{Our Prevention:} Always acquire in same order: \texttt{queue\_lock} → \texttt{read\_try} → \texttt{mutex} → \texttt{resource\_lock}

\subsubsection{Challenge 2: Spurious Wakeups}

Though we don't use condition variables in current implementation (could be future enhancement), production systems must handle spurious wakeups:

\begin{lstlisting}
// Always use while, not if!
while (condition_not_met) {
    pthread_cond_wait(&cond, &mutex);
}
\end{lstlisting}

\subsubsection{Challenge 3: Integer Overflow}

\textbf{Risk:} Theoretically, \texttt{active\_readers} could overflow with billions of readers.

\textbf{Mitigations:}
\begin{itemize}
    \item Use \texttt{unsigned long} or \texttt{uint64\_t} instead of \texttt{int}
    \item Add bounds checking: \texttt{if (active\_readers >= MAX\_READERS) return EAGAIN;}
    \item Our demo uses \texttt{int} for simplicity
\end{itemize}

\subsection{Performance Considerations}

\textbf{Lock Contention:}
\begin{itemize}
    \item \texttt{mutex} is hot - every reader and writer touches it
    \item Cache line bouncing on multi-core systems
    \item Solution: RCU (Read-Copy-Update) for read-heavy scenarios
\end{itemize}

\textbf{Context Switching:}
\begin{itemize}
    \item Blocking on mutex causes context switch (~1-10 microseconds)
    \item Multiplied by hundreds of operations = significant overhead
    \item Trade-off: Correctness vs. raw performance
\end{itemize}

\subsection{Thread-Safe Logging Implementation}

Our logger prevents interleaved output from concurrent threads:

\begin{lstlisting}
typedef struct {
    pthread_mutex_t log_mutex;
    bool initialized;
} logger_t;

void log_message(thread_type_t type, int id, const char* format, ...) {
    pthread_mutex_lock(&logger.log_mutex);
    
    // Generate timestamp
    struct timespec ts;
    clock_gettime(CLOCK_REALTIME, &ts);
    struct tm *tm_info = localtime(&ts.tv_sec);
    
    // Print atomic log entry
    printf("[%02d:%02d:%02d.%03ld] [%s%d] ",
           tm_info->tm_hour, tm_info->tm_min, tm_info->tm_sec,
           ts.tv_nsec / 1000000,
           type == THREAD_READER ? "R" : "W", id);
    
    va_list args;
    va_start(args, format);
    vprintf(format, args);
    va_end(args);
    
    printf("\n");
    fflush(stdout);
    
    pthread_mutex_unlock(&logger.log_mutex);
}
\end{lstlisting}

Without \texttt{log\_mutex}, output would be garbled:\begin{verbatim}
[12:34[12:34:56.123] :56.124] [R[W1] 2] read: wrote: "Hello""Wor
\end{verbatim}

\section{Experimental Evaluation}

\subsection{Test Environment}

\textbf{Hardware:} Multi-core processor with sufficient RAM

\textbf{Software:} Linux OS, GCC compiler, POSIX threads

\subsection{Comprehensive Automated Testing}

\textbf{Objective:} Validate system correctness through comprehensive testing.

\textbf{Test Configuration:}
\begin{itemize}
    \item Total runs: 16 (4 modes × 4 runs)
    \item Application: Shared String
    \item Writers per run: 8 threads
    \item Readers per run: 5 threads  
    \item Duration per run: 8 seconds
    \item Modes: vanilla, reader\_pref, writer\_pref, fair
\end{itemize}

\textbf{Automation:}
\begin{enumerate}
    \item \texttt{run\_tests.sh}: Executes tests, saves timestamped logs
    \item \texttt{analyze\_comprehensive.py}: Parses logs, detects torn reads
    \item Valid set: 20 predefined sentences (1-70 chars)
    \item Torn read = any string not in valid set
\end{enumerate}

\subsection{Actual Results}


\begin{table}[H]
\centering
\caption{Test Results - All 16 Runs}
\begin{tabular}{|l|r|r|}
\hline
\textbf{Mode} & \textbf{Clean Runs} & \textbf{Avg Torn Reads} \\ \hline
vanilla & 0/4 & 367 \\ \hline
reader\_pref & 4/4 & 0 \\ \hline
writer\_pref & 4/4 & 0 \\ \hline
fair & 4/4 & 0 \\ \hline
\end{tabular}
\end{table}

\textbf{Analysis:}

\textbf{Vanilla Mode:}
\begin{itemize}
    \item 0/4 clean runs = 100\% race condition rate
    \item Average 367 torn reads per run
    \item Examples: "Syncating systems..." (mixed), "Mutal exclusures..." (partial), single chars
    \item Clearly demonstrates concurrent write problem
\end{itemize}

\textbf{Synchronized Modes (reader\_pref, writer\_pref, fair):}
\begin{itemize}
    \item 12/12 runs clean = 100\% correctness
    \item Zero torn reads across all runs
    \item All read strings matched valid set perfectly
\end{itemize}

\textbf{Key Findings:}
\begin{enumerate}
    \item Perfect validation: All synchronized modes 100\% correct
    \item Clear problem demonstration: Vanilla 0\% success rate
    \item No false positives in any synchronized run
    \item Results reproducible across sessions
\end{enumerate}

\subsection{Statistical Significance}

With 16 total runs:
\begin{itemize}
    \item Vanilla mode: 0/4 clean = 0\% success rate (expected for unsynchronized)
    \item Synchronized modes: 12/12 clean = 100\% success rate
    \item Total operations: Hundreds of concurrent read/write operations per run
    \item No synchronization failures in 12 synchronized runs provides high confidence
\end{itemize}

\section{Discussion}

\subsection{Correctness vs. Performance}

Our experiments demonstrate a fundamental trade-off in concurrent systems design:

\begin{itemize}
    \item \textbf{Vanilla mode}: Maximum performance, zero correctness
    \item \textbf{Preferential modes}: Correct but unfair; can starve one side
    \item \textbf{Fair mode}: Correct and fair, with modest performance cost
\end{itemize}

For production systems, correctness is non-negotiable, making the real choice between preferential and fair policies.

\subsection{When to Use Each Mode}

\textbf{Reader Preference:} Suitable for read-heavy workloads where:
\begin{itemize}
    \item 90\%+ operations are reads
    \item Writes are infrequent and can tolerate delays
    \item Example: Configuration caches, reference data
\end{itemize}

\textbf{Writer Preference:} Suitable for write-heavy scenarios where:
\begin{itemize}
    \item Data freshness is critical
    \item Writes must complete promptly
    \item Example: Real-time monitoring, sensor data
\end{itemize}

\textbf{Fair Scheduling:} Suitable for mixed workloads where:
\begin{itemize}
    \item Both reads and writes are important
    \item Starvation cannot be tolerated
    \item Example: Shared services with SLAs
\end{itemize}

\subsection{Implementation Considerations}

Our implementation uses coarse-grained locking (single lock for entire resource). For higher scalability, production systems use:

\begin{itemize}
    \item \textbf{Fine-grained locking}: Multiple locks for different resource parts
    \item \textbf{Lock-free structures}: Atomic operations (compare-and-swap)
    \item \textbf{Read-Copy-Update (RCU)}: Used in Linux kernel
\end{itemize}

\section{Conclusion}

This paper presented a comprehensive study of the Reader-Writer synchronization problem through the implementation and evaluation of four distinct strategies. Our key findings include:

\begin{enumerate}
    \item \textbf{Vanilla mode clearly demonstrates the problem}: 100\% race condition rate with 367 torn reads per run
    
    \item \textbf{All synchronized modes achieve perfect correctness}: 100\% success rate (12/12 runs)
    
    \item \textbf{Preferential policies have trade-offs}: Reader preference maximizes read throughput but risks writer starvation. Writer preference ensures timely updates but may delay readers.
    
    \item \textbf{Fair scheduling eliminates starvation}: The turnstile pattern achieves balanced fairness with minimal overhead.
    
    \item \textbf{One size doesn't fit all}: The optimal synchronization strategy depends on workload characteristics, latency requirements, and fairness constraints.
\end{enumerate}

The unified API framework we developed enables easy comparison and switching between strategies, making it valuable for both education and prototyping. The shared string application demonstrates torn reads (data corruption) and the effectiveness of various synchronization approaches.

For practitioners, this work provides concrete guidance on selecting appropriate Reader-Writer implementations. For educators, our code serves as a clear demonstration of fundamental concurrency concepts using a straightforward shared string example. For researchers, it establishes a baseline for evaluating more sophisticated synchronization mechanisms.

\subsection{Future Directions}

Potential extensions include:
\begin{itemize}
    \item Priority-based scheduling with different thread priorities
    \item Adaptive algorithms that adjust strategy based on observed workload
    \item Lock-free implementations using atomic operations
    \item Performance evaluation on NUMA architectures with non-uniform memory access costs
\end{itemize}

The complete source code, including the shared string implementation with four synchronization modes, is available as open-source educational material.

\begin{thebibliography}{9}

\bibitem{tanenbaum2014}
Tanenbaum, A. S., \& Bos, H. (2014). 
\textit{Modern Operating Systems} (4th ed.). 
Pearson.

\bibitem{silberschatz2018}
Silberschatz, A., Galvin, P. B., \& Gagne, G. (2018).
\textit{Operating System Concepts} (10th ed.).
Wiley.

\bibitem{butenhof1997}
Butenhof, D. R. (1997).
\textit{Programming with POSIX Threads}.
Addison-Wesley Professional.

\bibitem{herlihy2012}
Herlihy, M., \& Shavit, N. (2012).
\textit{The Art of Multiprocessor Programming} (Revised 1st ed.).
Morgan Kaufmann.

\bibitem{courtoisbern1971}
Courtois, P. J., Heymans, F., \& Parnas, D. L. (1971).
Concurrent control with "readers" and "writers".
\textit{Communications of the ACM}, 14(10), 667-668.

\end{thebibliography}

\end{document}
